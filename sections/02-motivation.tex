%!TEX root = ../main.tex
\section{Motivation} 
\label{sec:motivation}

\noindent \textbf{User requirements.} Over the past several years, we have collaborated closely with over 200 enterprise customers from 16 different industries to better understand their big data processing requirements. Our analysis of key statistics has revealed the following insights:

\noindent \underline{\textit{Petabytes of data.}} Nearly half of our customers (49\%) have processed data ranging from one terabyte to 10 petabytes (PB). A significant percentage (29\%) handle more than 10 PB, while 8\% manage over 100 PB of data.

\noindent \underline{\textit{Data retention.}} In practice, 43\% customers are required to store data between 1 and 5 years. 22\% store between 5 and 10 years and 27\% store at least 10 years, according to regulations and practices in different industries.
 
 
\noindent \underline{\textit{Log data.}} A large majority (81\%) of our customers primarily work with log message data.

\noindent \underline{\textit{Stream and batch processing.}} Both stream and batch processing play a critical role in big data processing. 69\% of our customers actively use batch processing, and 65\% use stream processing. Nearly 40\% of customers care about both.


\noindent \textbf{System characteristics.} In summary, as businesses continue to generate  enormous amounts of data, it is essential to store petabytes of data for a long time, and leverage stream and batch processing techniques to process the data. Hence, the key abilities of the system that we want to provide to our customers are as follows:




\noindent \underline{\textit{High processing efficiency.}} The system should enable the rapid retrieval and analysis of large volumes of data in real-time. Also, it is also significant for customers with petabytes of data to efficiently to extend their business data pipelines to support new analytic requirements, which is always time-consuming because of the tools complexity in the big data stack.

%More than 71\% customers report that their platforms are required to support interactive queries for end user mobile apps and interactive business applications. Hence, highly efficient data processing is anticipated.

%Furthermore, enterprise customers working with petabytes of data report that it often requires weeks to months to extend the business data pipelines to support new analytic requirements due to the complexity of tools in the big data stack. Complete and flexible big data pipeline enterprise solutions are needed in order to shorten launch time in new analytic use cases.


\noindent \underline{\textit{High storage scalability.}}  Data volumes are expected to continue growing rapidly. For instance, as the number of 5G users increases, the amount of log data collected by our customers increases 5 times. The storage systems should be able to scale gracefully to store petabytes of new data.



\noindent \underline{\textit{Low cost.}} Flexible and cost-effective end-to-end solutions are important in real world deployment. For instance, some data centers of our customers still use 1 GE network although 10 GE network has already been a standard practice. When we deploy a new system to these data centers, the system should be able to remain its SLA even though facing network constraints. As many customers' IT budget growths are often slower than the exponential growth'of their data volumes and application requirements, our system design should take customer's previous IT investment into accounts and provide flexibilities to deploy in various datacenter infrastructures.

\cc{Not high level enough! cost or flexibility, which part?}


%Flexibility: Business requirements can change rapidly, so it's important to have a flexible infrastructure that can adapt quickly to changing needs.


\noindent \underline{\textit{High reliability.}} Data reliability and security remains a top priority in our customers' big data usage as any data loss or leak  could lead to serious damage. An enterprise data pipeline should provide built-in data reliability and security to provide full protection to the data.


\cc{Security seems not mentioned afterfords!}









%StreamLake has been successfully applied to a China Mobile data lake with production data, resulting in significant optimization of resource utilization. China Mobile manages one of the largest data analytic platforms in China, with over 4.8 petabytes of fresh business operation and network logging data flowing in from across 30 provinces and regions. As the platform grew to the exabyte scale, resource utilization became increasingly skewed, with average CPU, memory, and storage utilizations at 26%, 41%, and 66% respectively.




% The evaluation of the new system showed a 39\% reduction in servers needed to run the same number of analytic jobs, thanks to high utilization of data and server resources in StreamLake. In addition, the system also offered improved performance and service flexibility. For example, batch queries could be sped up by four times when the query operator pushdown and the LakeBrain features were enabled, and message streaming no longer required manual management of 300+ Kafka servers. The introduction of StreamLake also required minimal data migration for scaling, resulting in significantly reduced maintenance costs.






















