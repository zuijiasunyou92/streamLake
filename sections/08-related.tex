%!TEX root = ../main.tex
\section{Related Work} 
\label{sec:related}


In this section, we will discuss relevant open-source projects and systems related to \sys $w.r.t.$ existing data lake storage system, streaming platforms, lakehouse data management, and automatic database tuning.




\noindent\textbf{Data lake storage system.}  NetApp~\cite{netapp} supports Hadoop to use its storage devices through NFS-based connector docking, 
S3A docking to its object storage, and SAS/SCSI/FC building native HDFS~\cite{hdfs} on its block/lun devices. AWS~\cite{aws}, Azure~\cite{azure}, Google Cloud~\cite{google} and Ali Cloud~\cite{ali} provide a rich portfolio of storage services  to build data lake in the cloud. These cloud storage services are loosely connected to support messaging and batch processing while \sys tightly integrates message streaming, lakehouse and persistent storage in a single system which is more efficient and cost effective.


\noindent\textbf{Streaming platforms.} Kafka, Pulsar and Pravega~\cite{kafka, pulsa, pravega} are widely-used open-source streaming platforms in industry.  Unlike \sys, which builds its messaging service on top of the stream object and PLogs, and integrates its stream storage with a lakehouse framework, these solutions are file-based and require manual connections to compute engines and external storage, such as \hdfs~\cite{hdfs} or \texttt{S3}~\cite{s3} for downstream processing or cost-friendly archiving. This increases both the complexity and cost of data pipeline management.

%stream object



\noindent\textbf{Lakehouse.} Iceberg, Hudi and Delta Lake~\cite{iceberg,hudi,delta} are popular  lakehouse data management framework, which rely on statistic file or object storage. 
Massive data transmission between the storage and the compute engines are inevitable in many scenarios. \cc{Not coherent!!!}
 \sys builds the lakehouse framework on top of the table object and PLogs, leveraging the enterprise-level data redundancy, high performance cache and query computation pushdown to provide reliable and high speed concurrent lakehouse reads/writes. 
 
 %In addition, inspired by  methods proposed by~\cite{xiangyou, napa}, query operator pushdown can be used together with materialized views as cache to further optimize query performance.  This feature is on the roadmap to add to  \sys. 









%NetApp supports Hadoop by providing several connectors that enable Hadoop to use its storage devices. These connectors include an NFS-based connector for docking with NetApp's storage devices, an S3A connector for docking with NetApp's object storage, and native building of HDFS on NetApp's block/LUN devices through SAS/iSCSI/FC.

%AWS EMRFS is an enhancement introduced to address the inconsistency of object storage in metadata operations. Official information shows that AWS EMRFS has made computation pushdown related optimizations for the engine, which improve data access to persistent storage.

%Alibaba EMR is based on object storage, and it uses JindoFS to solve the performance problem of object storage by introducing local data caching. This solution improves data access to persistent storage in computation pushdown scenarios.

%StreamLake offers built-in computation pushdown operations directly, making it easy for users to take advantage of this feature without having to configure complex connectors or caching mechanisms.

%Overall, these solutions provide various ways to improve data access to persistent storage in computation pushdown scenarios, whether through native building of HDFS on block/LUN devices, improved metadata operations, or data caching.

\noindent\textbf{Automatic database tuning .} 
Recently, AI is widely-used inside the database system to improve the performance~\cite{survey,face, e2e, ottertune, ottertune2, cdbtune, qdtree, naru, deepdb}. For instances, OtterTune~\cite{ottertune} is a classic ML-based framework, recommending knob configuration using Gaussian process. Moreover, RL has been adopted in CDBTune~\cite{cdbtune} to iteratively explore the optimal configuration.
% Investigated in [41] shows the impact of the performance variation in production environments, indicating that GP tends to converge faster but is frequently trapped in local optima, whereas RL or deep learning (DL) generally needs a longer training process and achieves better performance.
   Sun et.al~\cite{skip} is the first approach that tries to maximize data skipping for a partitioning using pushdown predicates with a bottom-up approach. QD-tree~\cite{qdtree} proposed a greedy algorithm and a reinforcement learning based algorithm to further optimize the partitioning strategy.
    \cc{However, these algorithms need to quantify the performance of each candidate partitioning.} %In addition, the partitioning layout is sub-optimal when new data comes, as it is optimized based on existing data.