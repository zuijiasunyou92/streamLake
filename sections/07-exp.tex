%!TEX root = ../main.tex
\section{Experiment} 
\label{sec:exp}

\subsection{Experimental Settings}


\noindent \textbf{Our Experimental Scenario.} To showcase the effectiveness of the \sys framework, we employ a real-world use case simplified from the case in Section~\ref{sec:intro}.   A mobile financial application company collaborates with China Mobile to collect and analyze its app usage data. The company aims to understand its app usage patterns to prevent frauds and enhance its product experience. The mobile carrier provides this analytic service through an end-to-end big data processing pipeline. This pipeline includes several jobs such as data collection, normalization, labeling, and querying, as depicted in Figure~\ref{exp:fig:case}.
We compare  \sys framework with their current solution to build a big data processing pipeline that can facilitate business analysis.

\noindent \underline{\textit{(a) Collection:}}  The network carrier collects mobile app data packets in  data centers across the nation via deep packet inspection (DPI) and transfers them to a centralized storage pool.

\noindent \underline{\textit{(b) Normalization:}} At the  storage pool, the data packets are normalized as records in a unified schema. Data is validated to ensure accuracy and quality. Sensitive data is shielded to protect  privacy. 

\noindent \underline{\textit{(c) Labeling:}} Labels from knowledge bases are added, so as to classify the records and identify useful insights.

\noindent \underline{\textit{(d) Query:}} After the normalization and labeling processes are completed, the  records are inserted into tables and are available for query engines. To perform analyses, the app company employs secure API calls to query the data. Figure~\ref{exp:fig:sql} illustrates an example SQL query that counts the daily active users (DAU) in different provinces. More complicated analysis, like hidden Markov and Gaussian Mixture Models, can also be applied to draw user profiles and identify abnormal activities.

To support both full data and real time analyses, the network carrier builds two data flows in the pipeline. One flow processes full data in batch every two hours and the other processes stream messages constantly to deliver time-sensitive logs such as new logins, payments and password modifications. This ensures that the network carrier can effectively analyze both historical data and real-time events to make  accurate and timely decisions.

 \begin{figure}[htbp]
	\includegraphics[scale=0.35]{figures/sql}
	\centering
	\vspace{-1em}
	\caption{Query Example of Computing DAU.}
	\label{exp:fig:sql}
\end{figure}

\noindent \textbf{Settings.} This use case is evaluated in a  cluster using different sizes of input data packets and the results are compared with open-source storage solution Hadoop Distributed File System (\hdfs)~\cite{hdfs} and \kafka~\cite{kafka}. The reason of why we choose the two storage systems is that in reality, China Mobile has been using them for many years, which have shown stable and good performance. Hence, it is  reasonable to directly compare with the systems that our customer (China Mobile) is using. Also, in practice, many of our customers also use HDFS and Kafka to cope with similar application scenarios.


%\cc{For above, do we need more justification? like why hdfs and kafka fit? or common sense?}
%These two storage systems are chosen because of their popularity in real world and are relatively easy to provide context to illustrate the usage of StreamLake.


 
  To be specific, the cluster consists of 3 nodes, each with 24  2.30 GHz cores and 256 GB RAM. The cluster is configured as a 3-node \sys when we measure it.  While running the open-source solution, it is configured to host a 3-node \hdfs storage and a 3-node \kafka cluster simultaneously. The number of input data packets varies: 10 million, 50 million, 100 million, 500 million, and 1 billion packets. Each packet has an average size of 1.2 KB, resulting in corresponding data volumes of 12 GB, 60 GB, 120 GB, 600 GB, and 1.2 TB, respectively.

Overall, Figure~\ref{exp:fig:case} shows the data processing process.  \kafka and \hdfs serves as independent stream storage and batch storage respectively to pass data across collection, normalization, labeling and query jobs.
 As a typical ETL practice, a new copy of all data is written to \hdfs and \kafka after each job. In case  failing accidentally, a job can read its input data to reproduce the results.
 
 
 \begin{figure*}[htbp]
 	\includegraphics[scale=1.05]{figures/case}
 	\centering
 	\vspace{-1em}
 	\caption{Data Analytic Pipelines for a  Real-world Use Case.}
 	\label{exp:fig:case}
 \end{figure*}
 
  In our solution, \sys serves as a unified stream and batch processing storage. 
  We replace \kafka and \hdfs with \sys, which handles the message streaming and data storage. 
  Only minimal changes are made to the compute engines, so the business logic remains unchanged. 
  As \sys supports time travel, only updated rows are written to the storage. When a job needs to re-run, it can use time travel to retrieve its input data.  During the query jobs, for example, the three filters in the \texttt{WHERE} clause and the \texttt{COUNT} aggregate in Figure~\ref{exp:fig:sql} are pushed down to compute in \sys, so as to  accelerate the query.


\begin{table*}[ht]
	\begin{tabular}{|c|c|c|c|c|c|c|}
		\hline
		&  \#-Data Packet & 10,000,000 & 50,000,000 & 100,000,000 & 500,000,000 & 1,000,000,000 \\ \hline
		\multirow{3}{*}{Storage Space Usage (GB)}                  & \sys              & 34         & 166        & 329         & 1,659       & 3,289         \\ \cline{2-7} 
		& \hdfs + \kafka            & 145        & 729        & 1451        & 6,901       & 13,816        \\ \cline{2-7} 
		& Ratio (HK/S)         & 4.33       & 4.38       & 4.40        & 4.16        & 4.20          \\ \hline
		\multirow{3}{*}{Stream Processing Speed (Messages/Second)} & \sys              & 301,522    & 417,303    & 518,065     & 530,077     & 546,987       \\ \cline{2-7} 
		& \kafka             & 302,611    & 413,613    & 527,826     & 531,021     & 539,893       \\ \cline{2-7} 
		& Ratio (K/S)         & 1.00       & 0.99       & 1.02        & 1.00        & 0.99          \\ \hline
		\multirow{3}{*}{Batch Processing Total Time (Second)}      & \sys             & 259        & 664        & 1173        & 4868        & 9646          \\ \cline{2-7} 
		& \hdfs              & 212        & 795        & 1548        & 7535        & 14771         \\ \cline{2-7} 
		& Ratio (H/S)         & 0.82       & 1.19       & 1.32        & 1.55        & 1.53          \\ \hline
	\end{tabular}
	\caption{\sys v.s. \hdfs and \kafka.}\label{tab:case}
\end{table*}










\subsection{Overall Comparison}

Table~\ref{tab:case} shows the results. The numbers of input data packets are in the top row. The storage usage and processing time for \sys (S), \hdfs (H), \kafka (K) are in the following rows.  The ``Ratio'' represents that the ratio between \hdfs(\kafka) and \sys with respect to the storage usage or time. 
Note that HK denotes the sum of the storage usage in  \hdfs and \kafka.

The experiment demonstrates that \sys significantly improves the total storage cost and the batch processing time. The storage cost in the \hdfs and \kafka  is 4 times as much as \sys. The reason is that in \hdfs and \kafka, full data is written into the storage when each ETL job is finished, which is a common practice to support downstream jobs restart after unexpected failures. As a result, six copies of full data are written into the storage. 
While for our \sys, we save 75\%  storage cost by saving one copy of full data plus updates in each ETL job via the stream-to-table conversion and lakehouse functionality.

The batch processing speed in \sys is better than \hdfs when the workload is 50 million records or more.  As the workload grows,  \sys is 50\% faster than \hdfs when the workloads are 500 million and 1 billion records because we use the \brain optimizer and  write cache to improve the efficiency.
 On the other hand, \sys may not be the best choice for small workloads. When the workload is 10 million records, \sys is 20\% slower than \hdfs as it performs extra metadata management.
%the advantages of skipping irrelevant partitions and query operation pushdown become significant.
The message stream processing speed in \sys is competitive to \kafka. \sys and \kafka process about 300 thousand messages per second when the workload is 10 million records. Both systems scale to process about 500 thousand messages per second when the workloads are 100 million and more. 


\begin{figure*}
	\centering
	\includegraphics[width=\textwidth]{figures/streamengine}
	\caption{Evaluation of Message Streaming.}
	\label{fig:streamengine}
\end{figure*}



\subsection{Evaluation of Message Streaming}

To quantitively measure the message streaming service as an independent stream storage, we conduct an experiment to evaluate its throughput, latency, elasticity and volume. We select \texttt{OpenMessaging}~\cite{} as our benchmark  because it is widely used to compare messaging platforms. A cluster with three nodes is used in this experiment for  ease of reproduction.
 To help better understand the impact of tiering storage, two sets of hardware configurations are tested. In the first set of hardware (\texttt{Set-1}), each node has 10 CPU cores, 128 GB RAM and 800 GB NVMe SSD, 3 PB SAS HDD and all the nodes are connected with 10 Gb ethernet. In the second set of hardware (\texttt{Set-2}), all the configurations are the same except that each node has additional 16 GB persistent memory to serve as an extra cache. Messages are sent from producers to consumers in a fixed size of 1 KB. The data volumes we process are 100 TB, 500 TB and 1 PB respectively. 


Figure~\ref{fig:streamengine} shows the results in terms of  the latency, throughput, scaling time, and space consumption.
As shown in Figure~\ref{fig:streamengine}(b), persist memory reduces the latency as we expect, especially when the workload is 200k messages per second or less.
When it comes to the throughput (Figure~\ref{fig:streamengine}(a)), as the messages to process increase from 50000 per second to 1.5 million per second, the system throughput increases linearly. 
\texttt{Set-1} and \texttt{Set-2} achieve almost the same throughputs, indicating that it does not improve the throughput to add persistent memory as a cache. 
 Figure~\ref{fig:streamengine}(c) shows the high elasticity of the stream storage. The service gracefully scales from 1000 to 10000 partitions in less than 10 seconds. The good scalability  demonstrates a significant advantage of the   disaggregated storage architecture. 
Finally, Figure~\ref{fig:streamengine}(d) compares the space consumption different storage strategies (\texttt{Replication} refers to  saving data in its original format using multiple copies, \texttt{EC} refers to using erasure code to store the data, \texttt{EC+Col-store} refers to first converting the data to columnar format and then applying erasure code). 
The X-axis, $e.g.,$ \texttt{Fault Tolerance(FT)}=1 means that a storage cluster with the redundancy strategies can tolerant one node failure, and no data is lost.
The  Y-axis means  the times of its original data size   using these redundancy strategies.


 Without scarifying the reliability, \sys provides the options (\texttt{EC} and \texttt{EC+Col-store}) to use erasure coding and column-store which can save three to five times of storage cost compared to \texttt{Replication}. 






\begin{figure*}
	\centering
	\includegraphics[width=\textwidth]{figures/LakeBrain}
	\caption{\cc{LakeBrain Compaction and Partitioning Performance.}}
	\label{fig:lakebrain}
\end{figure*}



\subsection{Evaluation of \brain}

In this part, we evaluate the two components in \brain, $i.e.,$ auto-compaction and predicate-aware partitioning.

 
\noindent \textbf{Auto-Compaction:} To precisely evaluate the effectiveness of our automatic compaction strategy, a \texttt{TPC-H} based test bed is set up to ingest data from the message streaming platform to the data lake storage, during which a compaction strategy is tested.
 We run the experiment with 24 GB to 90 GB data and compare our \texttt{Auto-compaction} in \sys with \texttt{Default-compaction strategy}, $i.e.,$ a static strategy which simply compacts data files in a 30 second interval. 
During the ingestion, multiple rounds of \texttt{TPC-H} queries are executed in parallel  to obtain their end-to-end performance. As shown in Figure~\ref{fig:lakebrain}(a), the results depict how much improvement of  query performance that the compaction strategies can make, compared with the baseline.
 We can observe that the auto-compaction strategy outperforms the static one for all data volumes. As the data volume increases, the advantage becomes more significant because the number of blocks to be visited is reduced.




In addition to the query performance, we also evaluate the block utilization of the auto-compaction. Specifically, we control the file ingestion speed such that we can generate different numbers of files to measure both the run time and the block utilization in different workloads. The run time is evaluated along with the block utilization because an ideal strategy should improve the utilization without scarifying the performance. Similar to above, we deploy three methods: (1) No compaction, (2) The static strategy compacts data files in a 30 second interval, and (3) Auto-compaction. We can observe that the auto-compaction outperforms the static strategy in term of block utilization.  When we deploy the auto-compaction, the system is able to identify good compaction opportunities in which there are many small files and both the file ingestion speed and the block utilization are relatively low. 
File ingestion speed is important because compaction commits will fail if there are file access conflicts. As a comparison, it is hardly to avoid unnecessary or unsuccessful compaction in the static compaction strategy hence its performance is not good. Figure~\ref{fig:lakebrain}(b) shows the results of all three test groups.






\noindent \textbf{Predicate-Aware Partitioning:} We also test the partitioning method on \texttt{TPC-H}  with different scale factors. We train the probabilistic model with 3\% of the data randomly sampled from the \texttt{lineitem} table in a dataset generated with a scale factor of 2. After that, we obtain the optimized partitioning policy with the proposed method and evaluate our system on the full dataset with scale factors of 2, 5, 10 and 100. To evaluate the performance, we compare the resulting bytes skipped for \texttt{lineitem} table with (1) No partition (\texttt{full}). (2) Partition by the day of \texttt{l\_shipdate} (\texttt{day}).  (3) Our  method using sum-product networks~\cite{deepdb}  (\texttt{spn}) as the cardinality estimator.
 We compare the results with partitioning by the day of \texttt{l\_shipdate} considering it appears frequently in the pushdown predicates. The workload includes \texttt{TPC-H} query 6, 12 and 19 which involve \texttt{lineitem} table and include predicates other than \texttt{l\_shipdate}. We skip other \texttt{TPC-H} queries because their performance is  mainly dominated by the joins of multiple tables, which is beyond our purpose.
 
 \cc{Below is not clear enough!}

The results  in Figure~\ref{fig:lakebrain}(c,d) shows that the proposed method obtains non-marginal performance gains in terms of both \cc{bytes scanned} and the runtime. The fine-grained partitioning is superior on the queries in terms of data skipping compared to partitioning by the day of \texttt{l\_shipdate} because the optimized partitioning policy split the data based on other predicates except \texttt{l\_shipdate}. Even though the runtime for the queries is dominated by table joining, the optimized partitioning also demonstrates some improvement for query 6 and query 19, considering we only optimize the partition of the \texttt{lineitem} table. 



\begin{figure}
	\centering
	\includegraphics[width=\columnwidth]{figures/Querytime}
	\caption{Computation Pushdown Query Time.}
	\label{fig:pushdown}
\end{figure}



\subsection{Evaluation of Query Pushdown}
In this part, we evaluate the query operator pushdown  which we believe can provide stable query runtime regardless of network conditions. This is significant in the real-world deployment as it is  always expensive to upgrade the data center network. 
%In fact, many customers who we have worked with did the opposite to ask us to reuse their existing network to reduce the overall upgrade costs. Hence, it is a critical design that the query operator pushdown method ensures the query processing time and the application service level agreements even with a constraint network bandwidth.




In our experiment, we use two groups of clusters with different network bandwidths: one with 10 GB bandwidth and the other with 1 GB. To accurately assess the benefits of the query operator pushdown method, we carefully select three live queries with data-intensive operations and 4.8 TB of data from a China Mobile production environment. We deployed two different query engines, \texttt{Hive}~\cite{hive} and \texttt{Presto}~\cite{presto}, so as to process the SQL queries for generalization.

As shown in Figure~\ref{fig:pushdown} (the X-axis denotes the three commonly-used queries from our customers, and the Y-axis denotes the query time), 
we can observe that  without query operator pushdown, query performance of baselines varies  across engines. When  queries are executed in the 10 GB Ethernet, \texttt{Presto} completes the jobs in about 900 seconds, while \texttt{Hive} takes around 1200 seconds. When network bandwidth reduces to 1 GB, all execution times increases to over 3000 seconds.
 In comparison, when we apply query operator pushdown, the runtime of all the queries is close to 900 seconds, regardless of compute engines and network bandwidths, which indicates a 4 times performance advantage when the engine processes queries in an 1 GB bandwidth network.  In summary, we can conclude that our generalized query operator pushdown method introduces stable and high performance to query processing in a storage-disaggregated architecture. 



